{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">\n",
    "    <img src=\"img/logo_servicex.png\" width=\"70\" height=\"70\"  style=\"float:left\" alt=\"ServiceX\">\n",
    "    <img src=\"img/logo_ut.png\" width=\"150\" height=\"100\"  style=\"float:right\" alt=\"UT Austin\">\n",
    "    ServiceX, the novel data delivery system\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\">KyungEon Choi (UT Austin) for ServiceX team (IRIS-HEP)</h4>\n",
    "\n",
    "<h4 style=\"text-align: center;\">IRIS-HEp Analysis Software Training Event (July 19, 2024)</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data delivery? Data Access?</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<p style=\"text-align:center;\"> <img src=\"img/remote_data.png\"  width =\"60%\" alt=\"ServiceX\"></p>\n",
    "\n",
    "- Data we want to process is often stored at remote storages; sometimes too large to store directly accessible storage or production chain made it available only at remote storage\n",
    "- There are couple of solutions\n",
    "    1. Transfer or download to a directly accessible storage (e.g. <font size=\"2\">`rucio get X`</font>)\n",
    "    2. Run ntuplizer on the grid to filter and select what user need (and more), and then download (e.g. TopCPToolkit)\n",
    "    3. Go to the machine which has access to the data (e.g. lxplus for eos storage access)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>What is ServiceX?</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "    \n",
    "- A component of IRIS-HEP DOMA (Data Organization, Management And Access)\n",
    "- A scalable data extraction and delivery service\n",
    "- Deployed in a Kubernetes cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ServiceX under the hood</h3>\n",
    "<p style=\"text-align:center;\"> <img src=\"img/ServiceXDiagram2.png\" width=\"100%\" alt=\"ServiceX\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "- <b><span style=\"color:#FF6E33;\">Event data</span></b>\n",
    "    - ServiceX delivers from grid or remote XRootD storage to the user. Or more precisely ServiceX writes into an object store (ServiceX internal storage) and users download files or URLs from the object store as soon as available.\n",
    "    - Thickness of arrows reflect the amount of data over a wire. ServiceX is NOT designed to download full data from grids. Transformers effectively reduce data that will be delivered to user based on a query for selection and filtering.\n",
    "    - ServiceX is often co-located with a grid site to maximize network bandwith. XCache is preferable to allow much faster read for frequently accessed datasets.\n",
    "- <b><span style=\"color:red;\">Transformer</span></b>\n",
    "    - Extracts what user wants\n",
    "    - ServiceX consists of multiple microservices that are deployed as static K8s pod (always \"running\" state) but transformers are dynamically created via HPA (Horizontal Pod Scaling)\n",
    "    - A transformer pod runs on a file at a time and number of transformer pods are scaled up and down depending on the number of input files in the dataset and other criteria\n",
    "- <b>ServiceX Request</b>\n",
    "    - ServiceX request(s) is(are) made from the <span style=\"color:blue;\">SerivceX client libary</span> to ServiceX Web API via HTTP request\n",
    "    - A ServiceX request takes one input dataset (or list of files) and ServiceX is happily scale transformer pods automatically. A dataset with a single file should work but it's much more desirable to utilize HPA.\n",
    "    - Users can make ServiceX request anywhere only with Python ServiceX client library and <font size=\"2\"><code>servicex.yaml</code></font> includes an access token. Thus it's perfectly fine to deliver data to a university cluster or a laptop for small tests.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h3>ServiceX Webpage</h3>\n",
    "<font size=\"3\">\n",
    "    \n",
    "- The \"production\" ServiceX for ATLAS users: <font size=\"2\">[<code>https://servicex.af.uchicago.edu/</code>](https://servicex.af.uchicago.edu/)</font> - limited only to ATLAS users as it provides an access to the ATLAS event data\n",
    "- Download a ServiceX configuration file (<font size=\"2\"><code>servicex.yaml</code></font>) from the ServiceX website and copy to your home or working  directory \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"img/servicex_web.png\" width=\"80%\" alt=\"ServiceX Web\"></p>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ServiceX Client library</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "ServiceX Client library is a python library for users to communicate with ServiceX backend (or server) to make delivery requests and handling of outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "<b>Installation</b><br />\n",
    "- <font size=\"2\"><code>pip install servicex==3.0.0.alpha.19</code></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install servicex==3.0.0.alpha.19\n",
    "!pip list | grep servicex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>First ServiceX request</h3>\n",
    "\n",
    "<!-- <font size=\"3\">\n",
    "Let's begin with the basic: <br>\n",
    "<span style=\"margin-left:30px\">Deliver a branch (or column) from a dataset in the grid</span> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "<b>The most fundamental compenents of a ServiceX request</b>\n",
    "1. Dataset\n",
    "1. Query - describe what a user wants to run in transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import servicex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spec = {\n",
    "    \"Sample\":[{\n",
    "        \"Name\": \"UprootRaw\",\n",
    "        \"Dataset\": servicex.dataset.Rucio(\"user.kchoi.pyhep2024.test_dataset\"),\n",
    "        \"Query\": servicex.query.UprootRaw({\"treename\": \"nominal\", \"filter_name\": \"el_pt\"})\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "    \n",
    "- One sample named \"UprootRaw\" is defined in the <font size=\"2\"><code>spec</code></font> object.\n",
    "- A Rucio dataset is specified\n",
    "- Defined a <font size=\"2\">`Query`</font>, sent to transformers and run on all files in the given Rucio dataset\n",
    "- <font size=\"2\">`UprootRaw`</font> query takes <font size=\"2\">`\"treename\"`</font> to set <font size=\"2\">`TTree`</font> in flat ROOT ntuples and <font size=\"2\">`\"filter_name\"`</font> to select branches in a given tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Let's deliver my ServiceX request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o = servicex.deliver(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(o['UprootRaw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Returns a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Sample.Name: {o.keys()}\\n\")\n",
    "print(f\"Fileset: {type(o['UprootRaw'])}\\n\")\n",
    "print(f\"First file: {(o['UprootRaw'][0])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uproot\n",
    "\n",
    "with uproot.open(o['UprootRaw'][0]) as f:\n",
    "    column = f['nominal']['el_pt']\n",
    "column.array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Only few lines of a python script brings the data you want from the grid!\n",
    "\n",
    "<br></br>\n",
    "\n",
    "Let me go through what kinds of `Dataset` and `Query` are supported by ServiceX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "ServiceX supports Rucio, XRootD, and CERN OpenDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "servicex.dataset.Rucio.__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "servicex.dataset.FileList.__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "servicex.dataset.CERNOpenData.__init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Query</h3>\n",
    "\n",
    "<font size=\"3\">\n",
    "<ul>\n",
    "    <li>Query is a representation of what user wants from input dataset. e.g.</li>\n",
    "    <ul>\n",
    "        <li><font size=\"2\"><code>UprootRaw({\"treename\": \"nominal\", \"filter_name\": \"el_pt\"})</code></font></li>\n",
    "    </ul>\n",
    "    <li>User provided query is translated into a code that runs on transformers</li>\n",
    "    <li>Query is input data format dependent as a code for flat ROOT ntuple differs from the one for Apache parquet</li>\n",
    "    <!-- <li>ServiceX supports ROOT ntuples, ATLAS xAOD, CMS Run-1 AOD as an input format</li> -->\n",
    "    <!-- <li>Current version of client library supports query languages   (though other query classes are registered)</li> -->\n",
    "    <!-- <li>Current version of client library supports query classes for ROOT ntuples at the moment</li> -->\n",
    "</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "servicex.query.plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "<br>\n",
    "<b>Query classes for ROOT ntuples (via Uproot)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "<code>UprootRaw</code> Query\n",
    "- This is a new query language, essentially calling <font size=\"2\">`uproot.tree.arrays()`</font> function\n",
    "- A UprootRaw query can be a dictionary or a list of dictionaries\n",
    "- There are two types of operations a user can put in a dictionary\n",
    "    - query: contains a  <font size=\"2\">`treename`</font> key\n",
    "    - copy: contains a  <font size=\"2\">`copy_histograms`</font> key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"2\">    \n",
    "    <pre>\n",
    "        <code class=\"python\">\n",
    "query = [\n",
    "         {\n",
    "          'treename': 'reco', \n",
    "          'filter_name': ['/mu.*/', 'runNumber', 'lbn', 'jet_pt_*'], \n",
    "          'cut':'(count_nonzero(jet_pt_NOSYS>40e3, axis=1)>=4)'\n",
    "         },\n",
    "         {\n",
    "          'copy_histograms': ['CutBookkeeper*', '/cflow.*/', 'metadata', 'listOfSystematics']\n",
    "         }\n",
    "        ]\n",
    "        </code>\n",
    "    </pre>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "- More details on the grammar can be found [here](https://servicex-frontend.readthedocs.io/en/latest/transformer_matrix.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_UprootRaw = servicex.query.UprootRaw({\"treename\": \"nominal\", \"filter_name\": \"el_pt\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<code>FuncADL_Uproot</code> Query\n",
    "- Functional Analysis Description Language is a powerful query language that has been supported by ServiceX\n",
    "- In addition to the basic operations like <font size=\"2\">`Select()`</font> for column selection or <font size=\"2\">`Where()`</font> for filtering, more sophisticated query can be built\n",
    "- One new addition <font size=\"2\">`FromTree()`</font> method to set a tree name in a query\n",
    "- More details can be found at the [talk](https://indico.cern.ch/event/1019958/timetable/#31-funcadl-functional-analysis) by M. Proffitt at PyHEP 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_FuncADL = servicex.query.FuncADL_Uproot().FromTree('nominal').Select(lambda e: {'el_pt': e['el_eta']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<code>PythonFunction</code> Query\n",
    "- Python function can be passed as a query\n",
    "- <font size=\"2\">`uproot`</font>, <font size=\"2\">`awkward`</font>, <font size=\"2\">`vector`</font> can be imported (limited by the transformer image)\n",
    "- Primarily experimental purpose and likely to be discontinued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_query(input_filenames=None):\n",
    "    import uproot\n",
    "    with uproot.open({input_filenames: \"nominal\"}) as o:\n",
    "        br = o.arrays(\"el_pt\")\n",
    "    return br\n",
    "\n",
    "query_PythonFunction = servicex.query.PythonFunction().with_uproot_function(run_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "All three queries return the same output, ROOT files with selected branch <font size=\"2\"><code>el_pt</code></font>!\n",
    "\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Multiple samples</h3>\n",
    "\n",
    "<font size=\"3\">\n",
    "\n",
    "- HEP analysis often needs more than one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spec_multiple = {    \n",
    "    \"Sample\":[\n",
    "        {\n",
    "            \"Name\": \"UprootRaw\",\n",
    "            \"Dataset\": servicex.dataset.Rucio(\"user.kchoi.pyhep2024.test_dataset\"),\n",
    "            \"Query\": query_UprootRaw,\n",
    "        },\n",
    "        {\n",
    "            \"Name\": \"FuncADL_Uproot\",\n",
    "            \"Dataset\": servicex.dataset.Rucio(\"user.kchoi.pyhep2024.test_dataset\"),\n",
    "            \"Query\": query_FuncADL,\n",
    "        },\n",
    "        {\n",
    "            \"Name\": \"PythonFunction\",\n",
    "            \"Dataset\": servicex.dataset.Rucio(\"user.kchoi.pyhep2024.test_dataset\"),\n",
    "            \"Query\": query_PythonFunction,\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "- <font size=\"2\">`Sample`</font> block is a list of dictionaries, each with a <font size=\"2\">`Dataset`</font> - <font size=\"2\">`Query`</font> pair\n",
    "- Client library makes one ServiceX request per <font size=\"2\">`Dataset`</font> - <font size=\"2\">`Query`</font> pair\n",
    "- Again, it's preferred to have more files in a request to utilize K8s HPA than having multiple requests for the same query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o_multiple = servicex.deliver(spec_multiple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>YAML interface</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "- It's cool to deliver only interested columns from grid storages in a Jupyter notebook, but real analysis often becomes quite messy\n",
    "- A YAML file represents all of your data in your analysis and easily share with your colleague\n",
    "- The new client library brings <font size=\"2\">`servicex-databinder`</font> and significantly improve user interface to allow a seamless experience with YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile -a config_UprootRaw.yaml\n",
    "\n",
    "Sample:\n",
    "  - Name: Uproot_UprootRaw_YAML\n",
    "    Dataset: !Rucio user.kchoi.pyhep2024.test_dataset\n",
    "    Query: !UprootRaw |\n",
    "        {\"treename\":\"nominal\", \"filter_name\": \"el_pt\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Compare with the one in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\"Sample\":[{\n",
    "    \"Name\": \"UprootRaw_PyHEP\",\n",
    "    \"Dataset\": Rucio(\"user.kchoi.pyhep2024.test_dataset\"),\n",
    "    \"Query\": UprootRaw({\"treename\": \"nominal\", \"filter_name\": \"el_pt\"})\n",
    "}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from servicex import deliver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o_yaml = deliver(\"config_UprootRaw.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "YAML syntax\n",
    "- The exclamation mark(!), yaml tag, to declare dataset type and query type (see detail on the [PyYAML constructor](https://matthewpburruss.com/post/yaml/))\n",
    "    - Dataset tags: <font size=\"2\">`!Rucio`</font>, <font size=\"2\">`!Rucio`</font>, <font size=\"2\">`!FileList`</font>, <font size=\"2\">`!CERNOpenData`</font>\n",
    "    - Query tags: <font size=\"2\">`!UprootRaw`</font>, <font size=\"2\">`!FuncADL_Uproot`</font>, <font size=\"2\">`!PythonFunction`</font>\n",
    "- The pipe (`|`) after query tag represents the literal operator and allows to properly interpret multi-line string\n",
    "\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Optional configurations</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "- `General` block\n",
    "    - Optional block\n",
    "    - By default <font size=\"2\">`OutputFormat: root-file`</font>\n",
    "    - <font size=\"2\">`parquet`</font> is supported as <font size=\"2\">`OutputFormat`</font> for uproot queries except <font size=\"2\">`UprootRaw`</font>\n",
    "    - By default <font size=\"2\">`Delivery: LocalCache`</font> &rarr; files are downloaded to your local cache directory<sup>1</sup>\n",
    "    - Or <font size=\"2\">`Delivery: SignedURLs`</font> only returns ServiceX object-store URLs &rarr; user can consume data directly from the ServiceX object-store\n",
    "- `Sample` block\n",
    "    - <font size=\"2\">`NFiles`</font> to set number of files you want to run in the given Rucio dataset\n",
    "- `Definition` block\n",
    "    - Repeated long values can be replaced by setting YAML anchors, e.g. the same query for multiple samples\n",
    "    - One constraint is the anchor (<font size=\"2\">`&`</font>) needs to be defined prior to the alias (<font size=\"2\">`*`</font>)\n",
    "\n",
    "<font size=\"2\"><sup>1</sup>The local cache path can be set in the `servicex.yaml` file: `cache_path: /X/Y`</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Example YAML:\n",
    "</font>\n",
    "\n",
    "```yaml\n",
    "Definition:\n",
    "  - &DEF_ggH_input \"root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets\\\n",
    "                  /2020-01-22/4lep/MC/mc_345060.ggH125_ZZ4lep.4lep.root\"\n",
    "\n",
    "  - &DEF_query1 !PythonFunction |\n",
    "    def run_query(input_filenames=None):\n",
    "        import uproot\n",
    "\n",
    "        with uproot.open({input_filenames:\"nominal\"}) as o:\n",
    "            br = o.arrays(\"mu_pt\")\n",
    "        return br\n",
    "\n",
    "  - &DEF_query2 !FuncADL_Uproot  |\n",
    "    FromTree('mini').Select(lambda e: {'lep_pt': e['lep_pt']}).Where(lambda e: e['lep_pt'] > 1000)\n",
    "\n",
    "General:\n",
    "  OutputFormat: parquet\n",
    "  Delivery: SignedURLs\n",
    "\n",
    "Sample:\n",
    "  - Name: ttH\n",
    "    Dataset: !Rucio user.kchoi.fcnc_tHq_ML.ttH.v11\n",
    "    Query: *DEF_query1\n",
    "    NFiles: 5\n",
    "\n",
    "  - Name: ttZ\n",
    "    Dataset: !Rucio user.kchoi.fcnc_tHq_ML.ttZ.v11    \n",
    "    Query: *DEF_query1\n",
    "    NFiles: 3\n",
    "\n",
    "  - Name: ggH\n",
    "    Dataset: !FileList *DEF_ggH_input\n",
    "    Query: *DEF_query2\n",
    "```\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Failed transformation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_typo = {\n",
    "    \"Sample\":[{\n",
    "        \"Name\": \"UprootRaw_failed\",\n",
    "        \"Dataset\": servicex.dataset.Rucio(\"user.kchoi.pyhep2024.test_dataset\"),\n",
    "        \"Query\": servicex.query.UprootRaw({\"treename\": \"nominal\", \"filter_name\": \"el_pta\"})\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = deliver(spec_typo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Example use case</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\"> <img src=\"img/ServiceXDiagram2.png\" width=\"100%\" alt=\"ServiceX\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "<b>Case 1</b>\n",
    "- My analysis team's AnalysisTop (or TopCPToolkits) ntuples on two datasets (Rucio DIDs) are ready on the grid\n",
    "- I want all electron branches with electron pT > 25 GeV cut\n",
    "- I gonna do my analysis in the UC AF coffea-casa so I don't want to download to my local cache space than simply consume from the object store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spec_case1 = {\n",
    "    \"General\":\n",
    "    {\n",
    "        \"Delivery\": \"SignedURLs\"\n",
    "    },\n",
    "    \"Sample\":[\n",
    "        {\n",
    "            \"Name\": \"ttH\",\n",
    "            \"Dataset\": servicex.dataset.Rucio(\"user.kchoi.fcnc_tHq_ML.ttH.v11\"),\n",
    "            \"Query\": servicex.query.UprootRaw({\n",
    "                \"treename\":\"nominal\", \n",
    "                \"filter_name\": [\"el_*\", \"mu_*\",\"jet_*\"], \n",
    "                \"cut\": \"num(el_pt, axis=1)==3\"\n",
    "            })\n",
    "        },\n",
    "        {\n",
    "            \"Name\": \"ttW\",\n",
    "            \"Dataset\": servicex.dataset.Rucio(\"user.kchoi.fcnc_tHq_ML.ttW.v11\"),\n",
    "            \"Query\": servicex.query.UprootRaw({\n",
    "                \"treename\":\"nominal\", \n",
    "                \"filter_name\": [\"el_*\", \"mu_*\",\"jet_*\"], \n",
    "                \"cut\": \"num(el_pt, axis=1)==3\"\n",
    "            })\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o_case1 = servicex.deliver(spec_case1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o_case1['ttH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<font size=\"3\">\n",
    "\n",
    "<b>Case 2</b>\n",
    "- My analysis team stores all ntuples at EOS ATLAS space\n",
    "- I just want a few branches from all files in parquet format for my machine learning study\n",
    "- I want to deliver branches to my university cluster as it has a good GPU card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eos_file = servicex.dataset.FileList([\"root://eosuser.cern.ch//eos/atlas/atlascerngroupdisk/phys-higgs/HSG1/HZG/Run2/ProcessedSample/H2Zy-FullRun2-v3/data/data15_p3876_all.root\",\n",
    "                                     \"root://eosuser.cern.ch//eos/atlas/atlascerngroupdisk/phys-higgs/HSG1/HZG/Run2/ProcessedSample/H2Zy-FullRun2-v3/data/data17_p3876_all.root\",\n",
    "                                     \"root://eosuser.cern.ch//eos/atlas/atlascerngroupdisk/phys-higgs/HSG1/HZG/Run2/ProcessedSample/H2Zy-FullRun2-v3/data/data18_p3876_all.root\"])\n",
    "\n",
    "spec_case2 = {\n",
    "    \"Sample\":[\n",
    "        {\n",
    "            \"Name\": \"UprootRaw_eos\",\n",
    "            \"Dataset\": eos_file,\n",
    "            \"Query\": servicex.query.UprootRaw({\"treename\": \"HZG_Tree\", \"filter_name\": \"ph_*\"})\n",
    "        }    \n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o_case1 = servicex.deliver(spec_case2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o_case1[\"UprootRaw_eos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<font size=\"3\">\n",
    "\n",
    "<b>Case 3</b>\n",
    "- Sample game uses 6 processes and 1 file per each, but in practice we need to process all files :)\n",
    "    - <font size=\"2\">`HWW`: `mc20_13TeV.345324.PowhegPythia8EvtGen_NNLOPS_NN30_ggH125_WWlvlv_EF_15_5.deriv.DAOD_PHYSLITE.e5769_s3681_r13167_r13146_p6026_tid37865929_00` (20 files / 21GB)</font>\n",
    "    - <font size=\"2\">`HZZ`: `mc20_13TeV.345060.PowhegPythia8EvtGen_NNLOPS_nnlo_30_ggH125_ZZ4l.deriv.DAOD_PHYSLITE.e7735_s3681_r13167_r13146_p6026_tid38191712_00` (17 files / 19GB)</font>\n",
    "    - <font size=\"2\">`tcha`:`mc20_13TeV.410658.PhPy8EG_A14_tchan_BW50_lept_top.deriv.DAOD_PHYSLITE.e6671_s3681_r13167_r13146_p6026_tid37621204_00` (103 files / 230GB)</font>\n",
    "    - <font size=\"2\">`ttbar`: `mc20_13TeV.410470.PhPy8EG_A14_ttbar_hdamp258p75_nonallhad.deriv.DAOD_PHYSLITE.e6337_s3681_r13167_r13146_p6026_tid37620644_00` (547 files / 836GB)</font>\n",
    "    - <font size=\"2\">`tZq`: `mc20_13TeV.410560.MadGraphPythia8EvtGen_A14_tZ_4fl_tchan_noAllHad.deriv.DAOD_PHYSLITE.e5803_s3681_r13167_r13146_p6026_tid38191575_00` (15 files / 13GB)</font>\n",
    "    - <font size=\"2\">`Zee`: `mc20_13TeV.700322.Sh_2211_Zee_maxHTpTV2_CVetoBVeto.deriv.DAOD_PHYSLITE.e8351_s3681_r13167_r13146_p6026_tid37621317_00` (49 files / 71GB)</font>\n",
    "- The same list of branches:\n",
    "  <font size=\"2\">\n",
    "  \n",
    "  ```\n",
    "    \"EventInfoAuxDyn.mcEventWeights\",\n",
    "        \n",
    "    \"AnalysisElectronsAuxDyn.pt\",\n",
    "    \"AnalysisElectronsAuxDyn.eta\",\n",
    "    \"AnalysisElectronsAuxDyn.phi\",\n",
    "    \"AnalysisElectronsAuxDyn.m\",\n",
    "    \n",
    "    \"AnalysisMuonsAuxDyn.pt\",\n",
    "    \"AnalysisMuonsAuxDyn.eta\",\n",
    "    \"AnalysisMuonsAuxDyn.phi\",\n",
    "    \n",
    "    \"AnalysisJetsAuxDyn.pt\",\n",
    "    \"AnalysisJetsAuxDyn.eta\",\n",
    "    \"AnalysisJetsAuxDyn.phi\",\n",
    "    \"AnalysisJetsAuxDyn.m\",\n",
    "    \n",
    "    \"BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pb\",\n",
    "    \"BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pc\",\n",
    "    \"BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pu\",\n",
    "  ```\n",
    "  \n",
    "  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile -a config_sample_game.yaml\n",
    "\n",
    "Definition:\n",
    "  - &DEF_query !UprootRaw |\n",
    "        {\n",
    "            \"treename\": \"CollectionTree\",\n",
    "            \"filter_name\": [\n",
    "                \"EventInfoAuxDyn.mcEventWeights\",  \n",
    "                \"AnalysisElectronsAuxDyn.pt\",\n",
    "                \"AnalysisElectronsAuxDyn.eta\",\n",
    "                \"AnalysisElectronsAuxDyn.phi\",\n",
    "                \"AnalysisElectronsAuxDyn.m\",\n",
    "\n",
    "                \"AnalysisMuonsAuxDyn.pt\",\n",
    "                \"AnalysisMuonsAuxDyn.eta\",\n",
    "                \"AnalysisMuonsAuxDyn.phi\",\n",
    "\n",
    "                \"AnalysisJetsAuxDyn.pt\",\n",
    "                \"AnalysisJetsAuxDyn.eta\",\n",
    "                \"AnalysisJetsAuxDyn.phi\",\n",
    "                \"AnalysisJetsAuxDyn.m\",\n",
    "\n",
    "                \"BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pb\",\n",
    "                \"BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pc\",\n",
    "                \"BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pu\"]\n",
    "        }\n",
    "\n",
    "General:\n",
    "    Delivery: SignedURLs\n",
    "    \n",
    "Sample:\n",
    "  - Name: HWW\n",
    "    Dataset: !Rucio mc20_13TeV.345324.PowhegPythia8EvtGen_NNLOPS_NN30_ggH125_WWlvlv_EF_15_5.deriv.DAOD_PHYSLITE.e5769_s3681_r13167_r13146_p6026_tid37865929_00\n",
    "    Query: *DEF_query\n",
    "  - Name: HZZ\n",
    "    Dataset: !Rucio mc20_13TeV.345060.PowhegPythia8EvtGen_NNLOPS_nnlo_30_ggH125_ZZ4l.deriv.DAOD_PHYSLITE.e7735_s3681_r13167_r13146_p6026_tid38191712_00\n",
    "    Query: *DEF_query\n",
    "  - Name: tcha\n",
    "    Dataset: !Rucio mc20_13TeV.410658.PhPy8EG_A14_tchan_BW50_lept_top.deriv.DAOD_PHYSLITE.e6671_s3681_r13167_r13146_p6026_tid37621204_00\n",
    "    Query: *DEF_query\n",
    "  - Name: ttbar\n",
    "    Dataset: !Rucio mc20_13TeV.410470.PhPy8EG_A14_ttbar_hdamp258p75_nonallhad.deriv.DAOD_PHYSLITE.e6337_s3681_r13167_r13146_p6026_tid37620644_00\n",
    "    Query: *DEF_query\n",
    "  - Name: tZq\n",
    "    Dataset: !Rucio mc20_13TeV.410560.MadGraphPythia8EvtGen_A14_tZ_4fl_tchan_noAllHad.deriv.DAOD_PHYSLITE.e5803_s3681_r13167_r13146_p6026_tid38191575_00\n",
    "    Query: *DEF_query\n",
    "  - Name: Zee\n",
    "    Dataset: !Rucio mc20_13TeV.700322.Sh_2211_Zee_maxHTpTV2_CVetoBVeto.deriv.DAOD_PHYSLITE.e8351_s3681_r13167_r13146_p6026_tid37621317_00\n",
    "    Query: *DEF_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "o_case3 = servicex.deliver(\"config_sample_game.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Future plans</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>Client library</b>\n",
    "- Migrate ATLAS FuncADL queries\n",
    "- Improve robustness: progress bar (transform status/object store access) and local caching\n",
    "- Readthedoc of the new ServiceX cilent library is under construction! https://servicex-frontend.readthedocs.io/en/latest/index.html\n",
    "- ServiceX as a node of dask task graph\n",
    "\n",
    "<b>ServiceX backend</b>\n",
    "- Improve stability and robustness of ServiceX especially from what learned during 200Gbps challenge\n",
    "- Server-side caching\n",
    "- Add new ServiceX transformers: ATLAS TopCPToolkit transformer (WIP), column-join transformer, ATLAS columnar CP transformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
